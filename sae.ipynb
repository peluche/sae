{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with SAE\n",
    "https://transformer-circuits.pub/2023/monosemantic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package               Version\n",
      "--------------------- ------------\n",
      "accelerate            0.29.3\n",
      "aiohttp               3.9.5\n",
      "aiosignal             1.3.1\n",
      "appdirs               1.4.4\n",
      "appnope               0.1.4\n",
      "asttokens             2.4.1\n",
      "attrs                 23.1.0\n",
      "beartype              0.14.1\n",
      "better-abc            0.0.3\n",
      "cattrs                23.1.2\n",
      "certifi               2024.2.2\n",
      "charset-normalizer    3.3.2\n",
      "click                 8.1.7\n",
      "comm                  0.2.2\n",
      "contourpy             1.2.1\n",
      "cycler                0.12.1\n",
      "datasets              2.19.0\n",
      "debugpy               1.8.1\n",
      "decorator             5.1.1\n",
      "dill                  0.3.8\n",
      "docker-pycreds        0.4.0\n",
      "docstring-to-markdown 0.12\n",
      "einops                0.8.0\n",
      "executing             2.0.1\n",
      "fancy-einsum          0.0.3\n",
      "filelock              3.14.0\n",
      "fonttools             4.51.0\n",
      "frozenlist            1.4.1\n",
      "fsspec                2024.3.1\n",
      "gitdb                 4.0.11\n",
      "GitPython             3.1.43\n",
      "huggingface-hub       0.22.2\n",
      "idna                  3.7\n",
      "iniconfig             2.0.0\n",
      "ipykernel             6.29.4\n",
      "ipython               8.24.0\n",
      "ipywidgets            8.1.2\n",
      "jaxtyping             0.2.28\n",
      "jedi                  0.19.1\n",
      "Jinja2                3.1.3\n",
      "jupyter_client        8.6.1\n",
      "jupyter_core          5.7.2\n",
      "jupyterlab_widgets    3.0.10\n",
      "kiwisolver            1.4.5\n",
      "lsprotocol            2023.0.0b1\n",
      "markdown-it-py        3.0.0\n",
      "MarkupSafe            2.1.5\n",
      "matplotlib            3.8.4\n",
      "matplotlib-inline     0.1.7\n",
      "mdurl                 0.1.2\n",
      "mpmath                1.3.0\n",
      "multidict             6.0.5\n",
      "multiprocess          0.70.16\n",
      "nest-asyncio          1.6.0\n",
      "networkx              3.3\n",
      "numpy                 1.26.4\n",
      "packaging             24.0\n",
      "pandas                2.2.2\n",
      "parso                 0.8.3\n",
      "pexpect               4.9.0\n",
      "pillow                10.3.0\n",
      "pip                   24.0\n",
      "platformdirs          4.2.1\n",
      "pluggy                1.3.0\n",
      "prompt-toolkit        3.0.43\n",
      "protobuf              4.25.3\n",
      "psutil                5.9.8\n",
      "ptyprocess            0.7.0\n",
      "pure-eval             0.2.2\n",
      "pyarrow               16.0.0\n",
      "pyarrow-hotfix        0.6\n",
      "Pygments              2.17.2\n",
      "pyparsing             3.1.2\n",
      "pytest                8.2.0\n",
      "python-dateutil       2.9.0.post0\n",
      "python-lsp-jsonrpc    1.1.2\n",
      "python-lsp-ruff       1.6.0\n",
      "python-lsp-server     1.9.0\n",
      "pytz                  2024.1\n",
      "PyYAML                6.0.1\n",
      "pyzmq                 26.0.2\n",
      "regex                 2024.4.28\n",
      "requests              2.31.0\n",
      "rich                  13.7.1\n",
      "safetensors           0.4.3\n",
      "sentencepiece         0.2.0\n",
      "sentry-sdk            2.0.1\n",
      "setproctitle          1.3.3\n",
      "setuptools            68.2.2.post0\n",
      "six                   1.16.0\n",
      "smmap                 5.0.1\n",
      "stack-data            0.6.3\n",
      "sympy                 1.12\n",
      "tokenizers            0.19.1\n",
      "torch                 2.3.0\n",
      "tornado               6.4\n",
      "tqdm                  4.66.2\n",
      "traitlets             5.14.3\n",
      "transformer-lens      1.14.0\n",
      "transformers          4.40.1\n",
      "typeguard             2.13.3\n",
      "typing_extensions     4.11.0\n",
      "tzdata                2024.1\n",
      "ujson                 5.8.0\n",
      "urllib3               2.2.1\n",
      "wandb                 0.16.6\n",
      "wcwidth               0.2.13\n",
      "widgetsnbextension    4.0.10\n",
      "xxhash                3.4.1\n",
      "yarl                  1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HookspecMarker.__call__() got an unexpected keyword argument 'warn_on_impl_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/transformer_lens/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hook_points\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evals\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpast_key_value_caching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     HookedTransformerKeyValueCache,\n\u001b[1;32m      6\u001b[0m     HookedTransformerKeyValueCacheEntry,\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/transformer_lens/utils.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytest\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/pytest/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version_tuple\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_code\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionInfo\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_assert_rewrite\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcacheprovider\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CaptureFixture\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/_pytest/assertion/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m truncate\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaferepr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saferepr\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/_pytest/assertion/util.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaferepr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saferepr\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaferepr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saferepr_unlimited\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# The _reprcompare attribute on the util module is used by the new assertion\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# interpretation code and assertion rewriter to detect this plugin was\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# loaded and in turn call the hooks defined here as part of the\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# DebugInterpreter.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m _reprcompare: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m, \u001b[38;5;28mobject\u001b[39m], Optional[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/_pytest/config/__init__.py:59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TerminalWriter\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhookspec\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutcomes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fail\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_pytest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutcomes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Skipped\n",
      "File \u001b[0;32m~/src/github.com/peluche/sae/.venv/lib/python3.11/site-packages/_pytest/hookspec.py:302\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytest_collection_finish\u001b[39m(session: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called after collection has been performed and modified.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    :param session: The pytest session object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    Any conftest plugin can implement this hook.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;129m@hookspec\u001b[39m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirstresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_impl_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOOK_LEGACY_PATH_ARG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpylib_path_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathlib_path_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytest_ignore_collect\u001b[39m(\n\u001b[1;32m    311\u001b[0m     collection_path: Path, path: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEGACY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, config: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True to prevent considering this path for collection.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    This hook is consulted for all files and directories prior to calling\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    directory cannot ignore itself!).\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;129m@hookspec\u001b[39m(firstresult\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytest_collect_directory\u001b[39m(path: Path, parent: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollector\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptional[Collector]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: HookspecMarker.__call__() got an unexpected keyword argument 'warn_on_impl_args'"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformer_lens\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "# print(t.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24576\n"
     ]
    }
   ],
   "source": [
    "class CFG():\n",
    "    sae_layer = 0\n",
    "    n_in = 3072\n",
    "    n_hidden = n_in * 8\n",
    "    batch_size = 16\n",
    "    max_context = 600\n",
    "\n",
    "cfg = CFG()\n",
    "print(cfg.n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "gpt2 = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "# logits, activations = gpt2.run_with_cache(\"Hello World\")\n",
    "gpt2_dataset = gpt2.load_sample_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = gpt2.run_with_cache(\"a black cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, gpt2, gpt2_dataset):\n",
    "        self.gpt2 = gpt2\n",
    "        self.gpt2_dataset = gpt2_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gpt2_dataset)\n",
    "\n",
    "    # @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.gpt2_dataset[idx]\n",
    "        tokens = self.gpt2.to_tokens(sentence['text'], padding_side=\"right\")\n",
    "        _, activations = self.gpt2.run_with_cache(tokens)\n",
    "        return activations['mlp_out', cfg.sae_layer]\n",
    "    \n",
    "dataset = Dataset(gpt2, gpt2_dataset)\n",
    "train_dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = gpt2.to_tokens([gpt2_dataset[1002]['text'], gpt2_dataset[0]['text']], padding_side=\"right\")\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_mlp_out(gpt2):\n",
    "#     filter_mlp_out = lambda name: (\"blocks.0.hook_mlp_out\" == name)\n",
    "#     gpt2.reset_hooks()\n",
    "#     cache = {}\n",
    "#     def forward_cache_hook(act, hook):\n",
    "#         cache[hook.name] = act.detach()\n",
    "#     gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "#     return cache\n",
    "\n",
    "# cache = hook_mlp_out(gpt2)\n",
    "# gpt2(gpt2_dataset[:10]['text'])\n",
    "# cache['blocks.0.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size=32, write_after=1000):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "        raise Exception(\"end execution\")\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    output = []\n",
    "    with t.no_grad():\n",
    "        for slice in tqdm(range(0, len(gpt2_dataset), batch_size)):\n",
    "            print(\"looking at slice\", slice)\n",
    "            if slice % write_after < batch_size and slice > 0:\n",
    "                print(\"writing to disk\")\n",
    "                t.save(t.cat(output, dim=0), f\"activations_{slice}.pt\")\n",
    "                output = []\n",
    "\n",
    "            if slice+batch_size >= len(gpt2_dataset):\n",
    "                sliced_dataset = gpt2_dataset[slice:]['text']\n",
    "            else:\n",
    "                sliced_dataset = gpt2_dataset[slice:slice+batch_size]['text']\n",
    "\n",
    "            try:\n",
    "                gpt2(sliced_dataset)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            activations_for_batch = cache['blocks.0.mlp.hook_post']\n",
    "            output.append(activations_for_batch)\n",
    "    # print(f\"{output=}\")\n",
    "    t.save(t.cat(output, dim=0), f\"activations_final.pt\")\n",
    "\n",
    "# create_dataset(write_after=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [fname for fname in os.listdir('./') if 'new_activations' in fname]\n",
    "activation_tensors = [t.load(fname, mmap=True) for fname in files[:5]]\n",
    "# activation_tensors = t.cat(activation_tensors, dim=0)\n",
    "# print(activation_tensors.shape) # should be (10000, 1024, 768)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3072])\n"
     ]
    }
   ],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, activation_tensors):\n",
    "        self.activation_tensors = activation_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.activation_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.activation_tensors[idx]\n",
    "\n",
    "concat = t.utils.data.ConcatDataset([Dataset(act) for act in activation_tensors]) \n",
    "train_dataloader = DataLoader(concat, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[0].shape) # should be (64, 1024, 768)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden):\n",
    "        super(SAE, self).__init__()\n",
    "        self.b_a = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_d = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_e = nn.Parameter(t.zeros(n_hidden))\n",
    "\n",
    "        self.W_e = nn.Linear(n_in, n_hidden, bias=False)\n",
    "        self.W_d = nn.Linear(n_hidden, n_in, bias=False)\n",
    "\n",
    "    def forward(self, act):\n",
    "        x = act + self.b_a\n",
    "        x = self.W_e(x) + self.b_e\n",
    "        x = F.relu(x)\n",
    "        hidden_activations = x\n",
    "        x = self.W_d(x) + self.b_d\n",
    "        return x, hidden_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mturbochardo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shardul/src/github.com/peluche/sae/wandb/run-20240501_141723-76729pyo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/turbochardo/SAE/runs/76729pyo' target=\"_blank\">comic-glitter-26</a></strong> to <a href='https://wandb.ai/turbochardo/SAE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/turbochardo/SAE' target=\"_blank\">https://wandb.ai/turbochardo/SAE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/turbochardo/SAE/runs/76729pyo' target=\"_blank\">https://wandb.ai/turbochardo/SAE/runs/76729pyo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a578af5f978f4b7098196e02f29fbc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f868acce3248f393ca62abf0ea800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13865352a7df4143a7edcf702c4dd421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28c24e05cac40e0bf13136f8788911f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cecca212964545afd1d00fb6839db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99faf5bd97eb45afbc6b0e8d2442e229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e67ed195e394ff9a0dda1e01381f089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee399f83af3f4e10b1a83e627c5c19eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e9028d5d4547eb9c033f0cf946785a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448dcb4545a4fe3884f13eb212aeccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1a0c8e94c44f1f82e853689eb7e6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fe4fc036e2482a98fd84cfd831da23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m             opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# TODO: validation loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# if wnb:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;66;03m# wandb.log({\"epoch\": epoch})\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, opt, dataloader, epochs, l1_factor, wnb)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m l1_factor \u001b[38;5;241m*\u001b[39m sparsity_loss\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wnb:\n\u001b[0;32m---> 15\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mreconstruction_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m     16\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparsity_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: sparsity_loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m     17\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SAE(n_in=cfg.n_in, n_hidden=cfg.n_hidden).to(device)\n",
    "opt = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train(model, opt, dataloader, epochs=100, l1_factor=0.1, wnb=True):\n",
    "    if wnb:\n",
    "        wandb.init(project='SAE')\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch[0].to(device)\n",
    "            logits, hidden_activations = model(batch)\n",
    "            reconstruction_loss = F.mse_loss(logits, batch)\n",
    "            sparsity_loss = hidden_activations.abs().mean()\n",
    "            loss = reconstruction_loss + l1_factor * sparsity_loss\n",
    "            if wnb:\n",
    "                wandb.log({\"reconstruction_loss\": reconstruction_loss.item()})\n",
    "                wandb.log({\"sparsity_loss\": sparsity_loss.item()})\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        # TODO: validation loss\n",
    "        # if wnb:\n",
    "            # wandb.log({\"epoch\": epoch})\n",
    "\n",
    "# train(model, opt, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.save(model.state_dict(), 'model_intermediate.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"a\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"The heart of a blue whale is so\" #.upper()\n",
    "\n",
    "for i in range(10):\n",
    "    logits = gpt2(input)\n",
    "    most_probable_token = logits[0, -1].argmax().item()\n",
    "    output = gpt2.to_single_str_token(most_probable_token)\n",
    "    input += output\n",
    "    print(input)\n",
    "\n",
    "# logits = gpt2(\"THE HEART OF A BLUE WHALE IS A BL\")\n",
    "# print(logits.shape)\n",
    "# most_probable_token = logits[0, -1].argmax().item()\n",
    "# gpt2.to_single_str_token(most_probable_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_question_dataset = [\n",
    "    {\n",
    "        \"fact\": \"The heart of a blue whale is so big, a human could swim through the arteries of its heart.\",\n",
    "        \"question\": \"Is it true that a human could swim through the arteries of a blue whale's heart?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Sloths can hold their breath for up to 40\",\n",
    "        \"question\": \"Can sloths really hold their breath for up to 40 minutes?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A snail can sleep for up to three years.\",\n",
    "        \"question\": \"Can a snail sleep for up to three years?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Penguins have knees.\",\n",
    "        \"question\": \"Do penguins really have knees?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The male emperor penguin protects the eggs from the cold.\",\n",
    "        \"question\": \"Is it the male emperor penguin that protects the eggs from the cold?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The largest land animal is the African elephant.\",\n",
    "        \"question\": \"Is the African elephant the largest land animal?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Kangaroos use their tails for balance.\",\n",
    "        \"question\": \"Do kangaroos use their tails for balance?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A group of flamingos is called a 'flamboyance.'\",\n",
    "        \"question\": \"Is a group of flamingos called a 'flamboyance'?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Rabbits and hares are different species.\",\n",
    "        \"question\": \"Are rabbits and hares different species?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Cows have four stomachs.\",\n",
    "        \"question\": \"Do cows have four stomachs?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "gpt2.reset_hooks()\n",
    "cache = {}\n",
    "def forward_cache_hook(act, hook):\n",
    "    with t.no_grad():\n",
    "        logits, hidden_activations = model(act)\n",
    "        cache[hook.name] = hidden_activations.detach()\n",
    "        return logits\n",
    "\n",
    "gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches = []\n",
    "upper_caches = []\n",
    "for pair in fact_question_dataset:\n",
    "    lower = pair[\"fact\"]\n",
    "    upper = lower.upper()\n",
    "    gpt2(lower)\n",
    "    lower_caches.append(cache[\"blocks.0.mlp.hook_post\"])\n",
    "    gpt2(upper)\n",
    "    upper_caches.append(cache[\"blocks.0.mlp.hook_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lower = t.stack([x.amax(dim=(0,1)) for x in lower_caches]).amax(0)\n",
    "print(max_lower)\n",
    "min_upper = t.stack([x.amin(dim=(0,1)) for x in upper_caches]).amin(0)\n",
    "print(min_upper.amax())\n",
    "neuron_idx = (min_upper - max_lower).argmax()\n",
    "print(min_upper[neuron_idx])\n",
    "print(max_lower[neuron_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.rand(3,5)\n",
    "print(a)\n",
    "print(a.topk(2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([111, 110, 110, 110, 106, 105, 104, 102, 102, 102, 102, 102, 102, 102,\n",
      "        102, 102, 102, 102, 102, 102, 102, 102, 102, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        100, 100, 100, 100, 100,  98,  97,  97,  97,  97,  94,  92,  91,  88,\n",
      "         71,  63]),\n",
      "indices=tensor([15663,  1391, 17581,  7686, 23529,  6271,   347,     8,     3,    11,\n",
      "         4037,     2,     1,     6,    10,     0,    12,     4,    22,    13,\n",
      "            9,     5,     7,    59,    29,    61,    57,    56,    27,    55,\n",
      "           53,    25,    51,    49,    48,    23,    47,    45,    21,    43,\n",
      "           44,    46,    20,    41,    24,    50,    52,    26,    54,    42,\n",
      "           40,    19,    39,    28,    58,    14,    60,    30,    62,    38,\n",
      "           18,    75,    73,    17,    35,    71,    72,    36,    74,    37,\n",
      "           70,    34,    69,    68,    16,    33,    67,    66,    32,    65,\n",
      "           64,    15,    31,    63,    80,    79,    78,    77,    76,    81,\n",
      "           85,    84,    83,    82, 15213,    86, 17570,    87,    88,  4843]))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in lower_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([166, 163, 161, 161, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
      "        160, 160, 160, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159,\n",
      "        159, 159, 159, 159, 159, 159, 159, 158, 158, 158, 158, 158, 158, 158,\n",
      "        158, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157,\n",
      "        157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 156, 155, 155,\n",
      "        155, 155, 155, 155, 154, 154, 154, 154, 154, 154, 154, 154, 154, 154,\n",
      "        154, 152, 151, 151, 150, 150, 149, 148, 147, 145, 140, 134, 129, 124,\n",
      "        116,  95]),\n",
      "indices=tensor([15663, 17581,  1391,  7686,    12,     5,     9,     6,     4,     2,\n",
      "           10,     3,     8,    11,     7,     1,     0,    28,    27,    25,\n",
      "           13,    24,    23,    26,    14,    21, 23529,    22,    19,    18,\n",
      "           17,    29,    16,    15,    20,    30,    37,    36,    35,    34,\n",
      "           33,    32,    31,    60,    59,    57,    56,    55,    53,    54,\n",
      "           52,    51,    58,    50,    49,    48,  6271,    47,    46,    45,\n",
      "           44,    43,    42,    41,    40,    39,    38,    61,    62,    67,\n",
      "           66,    65,    64,    63,    78,    77,    76,    75,    74,    73,\n",
      "           72,    71,    70,    69,    68,    79,   347,  4037,    81,    80,\n",
      "           82,    83,    84,    85,    86,    87, 15213,    88, 17570,    89]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in upper_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = gpt2(fact_question_dataset[0]['fact']) \n",
    "\n",
    "most_probable_token = t.argmax(output[0, -1]).item()\n",
    "gpt2.to_single_str_token(most_probable_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAE(n_in=cfg.n_in, n_hidden=cfg.n_hidden).to(device)\n",
    "model.load_state_dict(t.load(\"model_intermediate.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_neuron(neuron_idx, autoencoder, model, dataset): # dataset is a list of sentences\n",
    "   filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "   gpt2.reset_hooks()\n",
    "   cache = {}\n",
    "   def forward_cache_hook(act, hook):\n",
    "      cache[hook.name] = act.detach()\n",
    "      raise Exception(\"end execution\")\n",
    "   gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "   try:\n",
    "      gpt2(dataset)\n",
    "   except Exception:\n",
    "      pass\n",
    "\n",
    "   activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "   _, sae_activations = autoencoder(activations_for_batch)\n",
    "\n",
    "   activations_per_sentence_and_word = {}\n",
    "\n",
    "   print(sae_activations.shape)\n",
    "   tokenized = gpt2.to_tokens(dataset)\n",
    "   for sentence_idx, sentence in enumerate(dataset):\n",
    "      sentence_tokens = tokenized[sentence_idx]\n",
    "      for token_idx, token in enumerate(sentence_tokens):\n",
    "         activations_per_sentence_and_word[(sentence_idx, token_idx)] = sae_activations[sentence_idx, token_idx, neuron_idx]\n",
    "   return activations_per_sentence_and_word\n",
    "\n",
    "# neuron_0 = monitor_neuron(0, autoencoder=model, model=gpt2, dataset=gpt2_dataset[:32]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 24576])\n"
     ]
    }
   ],
   "source": [
    "# all_neuron_activations = [\n",
    "#     sorted(\n",
    "#         monitor_neuron(i, model, gpt2, gpt2_dataset[:32]['text']).items(),\n",
    "#         key=lambda x: x[1],\n",
    "#         reverse=True\n",
    "#     )\n",
    "# for i in [14000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>',\n",
      " '<|endoftext|>A',\n",
      " '<|endoftext|>A magazine',\n",
      " '<|endoftext|>A magazine supplement',\n",
      " '<|endoftext|>A magazine supplement with',\n",
      " '<|endoftext|>A magazine supplement with an',\n",
      " '<|endoftext|>A magazine supplement with an image',\n",
      " '<|endoftext|>A magazine supplement with an image of',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " 'title',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title '\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Un\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book'\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is pictured\",\n",
      " \"A magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in\",\n",
      " \" magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in Berlin\",\n",
      " \" supplement with an image of Adolf Hitler and the title 'The Unreadable \"\n",
      " \"Book' is pictured in Berlin.\",\n",
      " \" with an image of Adolf Hitler and the title 'The Unreadable Book' is \"\n",
      " 'pictured in Berlin. No',\n",
      " \" an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law',\n",
      " \" image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law bans',\n",
      " \" of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. \"\n",
      " 'No law bans ',\n",
      " \" Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No \"\n",
      " 'law bans ',\n",
      " \" Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law \"\n",
      " 'bans Me',\n",
      " \" and the title 'The Unreadable Book' is pictured in Berlin. No law bans \"\n",
      " 'Mein',\n",
      " \" the title 'The Unreadable Book' is pictured in Berlin. No law bans Mein \"\n",
      " 'Kamp',\n",
      " \" title 'The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \" 'The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \"The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \" Unreadable Book' is pictured in Berlin. No law bans Mein Kampf in\",\n",
      " \"readable Book' is pictured in Berlin. No law bans Mein Kampf in Germany\",\n",
      " \" Book' is pictured in Berlin. No law bans Mein Kampf in Germany,\",\n",
      " \"' is pictured in Berlin. No law bans Mein Kampf in Germany, but\",\n",
      " ' is pictured in Berlin. No law bans Mein Kampf in Germany, but the',\n",
      " ' pictured in Berlin. No law bans Mein Kampf in Germany, but the government',\n",
      " ' in Berlin. No law bans Mein Kampf in Germany, but the government of',\n",
      " ' Berlin. No law bans Mein Kampf in Germany, but the government of Bav',\n",
      " '. No law bans Mein Kampf in Germany, but the government of Bavaria',\n",
      " ' No law bans Mein Kampf in Germany, but the government of Bavaria,',\n",
      " ' law bans Mein Kampf in Germany, but the government of Bavaria, holds',\n",
      " ' bans Mein Kampf in Germany, but the government of Bavaria, holds the',\n",
      " ' Mein Kampf in Germany, but the government of Bavaria, holds the copyright',\n",
      " 'Mein Kampf in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and',\n",
      " 'Mein Kampf in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and guards',\n",
      " 'in Kampf in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it',\n",
      " ' Kampf in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it fer',\n",
      " 'f in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it feroc',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously.',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously. (',\n",
      " ' Germany, but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas',\n",
      " ', but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter',\n",
      " ' but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter/',\n",
      " ' the government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/RE',\n",
      " ' government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/REUTERS',\n",
      " ' of Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)',\n",
      " ' Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)\\n',\n",
      " 'aria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n',\n",
      " ', holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The',\n",
      " ' holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city',\n",
      " ' the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that',\n",
      " ' copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was',\n",
      " ' and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the',\n",
      " ' guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center',\n",
      " ' it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of',\n",
      " ' ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf',\n",
      " 'ociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler',\n",
      " 'iously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler',\n",
      " '. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler',\n",
      " ' (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitlers',\n",
      " 'Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire',\n",
      " ' Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitlers empire is',\n",
      " '/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered',\n",
      " 'REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with',\n",
      " 'UTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders',\n",
      " ')\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of',\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the',\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi',\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi past',\n",
      " ' city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi past,',\n",
      " ' that was the center of Adolf Hitlers empire is littered with reminders of '\n",
      " 'the Nazi past, from',\n",
      " ' was the center of Adolf Hitlers empire is littered with reminders of the '\n",
      " 'Nazi past, from the',\n",
      " ' the center of Adolf Hitlers empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet',\n",
      " ' center of Adolf Hitlers empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet holes',\n",
      " ' of Adolf Hitlers empire is littered with reminders of the Nazi past, from '\n",
      " 'the bullet holes that',\n",
      " ' Adolf Hitlers empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit',\n",
      " ' Hitlers empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit the',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many',\n",
      " ' empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many buildings',\n",
      " ' is littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to',\n",
      " ' littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to the',\n",
      " ' with reminders of the Nazi past, from the bullet holes that pit the fronts '\n",
      " 'of many buildings to the h']\n"
     ]
    }
   ],
   "source": [
    "# tokenized = gpt2.to_tokens(gpt2_dataset[:32]['text'])\n",
    "# tokens_which_activated_neuron_23 = all_neuron_activations[0][:100]\n",
    "# # print(tokens_which_activated_neuron_23[0])\n",
    "# tokenized_sentences = [\n",
    "#     tokenized[sentence_idx][max(word_idx - 20, 0):word_idx + 1] for (sentence_idx, word_idx), _ in tokens_which_activated_neuron_23\n",
    "# ]\n",
    "# untokenized_sentences = [\n",
    "#     gpt2.to_string(tokenized_sentence) for tokenized_sentence in tokenized_sentences\n",
    "# ]\n",
    "# from pprint import pprint\n",
    "# pprint(untokenized_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "gpt2.reset_hooks()\n",
    "cache = {}\n",
    "def forward_cache_hook(act, hook):\n",
    "    cache[hook.name] = act.detach()\n",
    "    raise Exception(\"end execution\")\n",
    "gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "try:\n",
    "    gpt2(gpt2_dataset[:32]['text'])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "_, sae_activations = model(activations_for_batch)\n",
    "neuron_stats = ((sae_activations > 1e-3).sum((0, 1)) < 10) & ((sae_activations > 1e-3).sum((0, 1)) >= 8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
