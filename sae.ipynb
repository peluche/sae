{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with SAE\n",
    "https://transformer-circuits.pub/2023/monosemantic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformer_lens\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if t.cuda.is_available() else ('mps' if t.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    sae_layer = 0\n",
    "    n_in = 3072\n",
    "    n_hidden = n_in * 8\n",
    "    batch_size = 16\n",
    "    max_context = 600\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2 = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "gpt2_dataset = gpt2.load_sample_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n"
     ]
    }
   ],
   "source": [
    "@t.no_grad()\n",
    "def scope():\n",
    "    logits, cache = gpt2.run_with_cache(\"a black cat\")\n",
    "    print(cache)\n",
    "\n",
    "scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, gpt2, gpt2_dataset):\n",
    "        self.gpt2 = gpt2\n",
    "        self.gpt2_dataset = gpt2_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gpt2_dataset)\n",
    "\n",
    "    # @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.gpt2_dataset[idx]\n",
    "        tokens = self.gpt2.to_tokens(sentence['text'], padding_side=\"right\")\n",
    "        _, activations = self.gpt2.run_with_cache(tokens)\n",
    "        return activations['mlp_out', cfg.sae_layer]\n",
    "    \n",
    "dataset = Dataset(gpt2, gpt2_dataset)\n",
    "train_dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_mlp_out(gpt2):\n",
    "#     filter_mlp_out = lambda name: (\"blocks.0.hook_mlp_out\" == name)\n",
    "#     gpt2.reset_hooks()\n",
    "#     cache = {}\n",
    "#     def forward_cache_hook(act, hook):\n",
    "#         cache[hook.name] = act.detach()\n",
    "#     gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "#     return cache\n",
    "\n",
    "# cache = hook_mlp_out(gpt2)\n",
    "# gpt2(gpt2_dataset[:10]['text'])\n",
    "# cache['blocks.0.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size=32, write_after=1000):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "        raise Exception(\"end execution\")\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    output = []\n",
    "    with t.no_grad():\n",
    "        for slice in tqdm(range(0, len(gpt2_dataset), batch_size)):\n",
    "            print(\"looking at slice\", slice)\n",
    "            if slice % write_after < batch_size and slice > 0:\n",
    "                print(\"writing to disk\")\n",
    "                t.save(t.cat(output, dim=0), f\"activations_{slice}.pt\")\n",
    "                output = []\n",
    "\n",
    "            if slice+batch_size >= len(gpt2_dataset):\n",
    "                sliced_dataset = gpt2_dataset[slice:]['text']\n",
    "            else:\n",
    "                sliced_dataset = gpt2_dataset[slice:slice+batch_size]['text']\n",
    "\n",
    "            try:\n",
    "                gpt2(sliced_dataset)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            activations_for_batch = cache['blocks.0.mlp.hook_post']\n",
    "            output.append(activations_for_batch)\n",
    "    # print(f\"{output=}\")\n",
    "    t.save(t.cat(output, dim=0), f\"activations_final.pt\")\n",
    "\n",
    "# create_dataset(write_after=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [fname for fname in os.listdir('./') if 'new_activations' in fname]\n",
    "activation_tensors = [t.load(fname, mmap=True) for fname in files[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "datasets should not be an empty iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_tensors[idx]\n\u001b[0;32m---> 11\u001b[0m concat \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConcatDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactivation_tensors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(concat, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscope\u001b[39m():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:317\u001b[0m, in \u001b[0;36mConcatDataset.__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(datasets)\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets should not be an empty iterable\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, IterableDataset), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatDataset does not support IterableDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
     ]
    }
   ],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, activation_tensors):\n",
    "        self.activation_tensors = activation_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.activation_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.activation_tensors[idx]\n",
    "\n",
    "concat = t.utils.data.ConcatDataset([Dataset(act) for act in activation_tensors]) \n",
    "train_dataloader = DataLoader(concat, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "def scope():\n",
    "    for batch in train_dataloader:\n",
    "        print(batch[0].shape) # should be (64, 1024, 768)\n",
    "        break\n",
    "scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden):\n",
    "        super(SAE, self).__init__()\n",
    "        self.b_a = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_d = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_e = nn.Parameter(t.zeros(n_hidden))\n",
    "\n",
    "        self.W_e = nn.Linear(n_in, n_hidden, bias=False)\n",
    "        self.W_d = nn.Linear(n_hidden, n_in, bias=False)\n",
    "\n",
    "    def forward(self, act):\n",
    "        x = act + self.b_a\n",
    "        x = self.W_e(x) + self.b_e\n",
    "        x = F.relu(x)\n",
    "        hidden_activations = x\n",
    "        x = self.W_d(x) + self.b_d\n",
    "        return x, hidden_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAE(n_in=cfg.n_in, n_hidden=cfg.n_hidden).to(device)\n",
    "opt = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train(model, opt, dataloader, epochs=100, l1_factor=0.1, wnb=True):\n",
    "    if wnb:\n",
    "        wandb.init(project='SAE')\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch[0].to(device)\n",
    "            logits, hidden_activations = model(batch)\n",
    "            reconstruction_loss = F.mse_loss(logits, batch)\n",
    "            sparsity_loss = hidden_activations.abs().mean()\n",
    "            loss = reconstruction_loss + l1_factor * sparsity_loss\n",
    "            if wnb:\n",
    "                wandb.log({\"reconstruction_loss\": reconstruction_loss.item()})\n",
    "                wandb.log({\"sparsity_loss\": sparsity_loss.item()})\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "# train(model, opt, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.save(model.state_dict(), 'model_intermediate.pt')\n",
    "# model.load_state_dict(t.load('model_intermediate.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart of a blue whale is so MUCH MORE THAN THE BODY.\n",
      "THE HEART OF A BLUE WHALE IS SO MUCH MORE THAN A PENNY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'THE HEART OF A BLUE WHALE IS SO MUCH MORE THAN A PENNY'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@t.no_grad()\n",
    "def autoregressively_generate(gpt2, input, n_tokens=8, print_step=False, print_final=True):\n",
    "    for i in range(n_tokens):\n",
    "        logits = gpt2(input)\n",
    "        most_probable_token = logits[0, -1].argmax().item()\n",
    "        output = gpt2.to_single_str_token(most_probable_token)\n",
    "        input += output\n",
    "        if print_step: print(input)\n",
    "    if print_final: print(input)\n",
    "    return input\n",
    "\n",
    "input = \"The heart of a blue whale is so\"\n",
    "autoregressively_generate(gpt2, input)\n",
    "autoregressively_generate(gpt2, input.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sae_hook(gpt2=gpt2, sae=model):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        with t.no_grad():\n",
    "            logits, hidden_activations = sae(act)\n",
    "            cache[hook.name] = hidden_activations.detach()\n",
    "            return logits\n",
    "\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YOU\n"
     ]
    }
   ],
   "source": [
    "input = 'the black cat was eating'.upper()\n",
    "input = 'once uppon a time in a'.upper()\n",
    "input = 'hello, my name is regis, how are'.upper()\n",
    "\n",
    "gpt2.reset_hooks()\n",
    "# names_filter=\n",
    "logits, cache = gpt2.run_with_cache(input)\n",
    "\n",
    "point = cache['blocks.0.hook_mlp_out']\n",
    "# point = cache['blocks.0.hook_resid_post']\n",
    "# point = cache['ln_final.hook_normalized']\n",
    "point = cache['blocks.0.hook_attn_out']\n",
    "point = cache['blocks.0.hook_resid_mid']\n",
    "\n",
    "point = cache['blocks.0.hook_resid_pre'] # earliest uppercase\n",
    "point = cache['blocks.7.hook_mlp_out'] # earliest MLP that is upper? sometime 6 ?\n",
    "\n",
    "\n",
    "tokens = gpt2.unembed(gpt2.ln_final(point)).softmax(dim=-1).argmax(dim=-1)\n",
    "print(gpt2.to_string(tokens[0, -1]))\n",
    "\n",
    "# autoregressively_generate(gpt2, input)\n",
    "# autoregressively_generate(gpt2, input.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_next_token=[' led', ' I', ' and', ' and', ' time', ' Health', ' is', ' What', ' and', ' the', ' a', ' World', ' to', ' the', ' be', ' to', ' for', ' and', ' the', ' the']\n",
      "mlp_outs=['um', 'ity', 'alia', 'med', 'lying', 'boy', 'um', 'ler', 'lets', 'ap', 'ae', 'ap', 'ae', 'ae', 'adem', 'ae', 'oe', 'ae', 'ler', 'unci']\n",
      "max activated token:  tensor(401)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scope(layer='blocks.7.mlp.hook_post'):\n",
    "    sentences = [\n",
    "        \"THIS IS THE BEGINNING OF A GREAT ADVENTURE, WHICH THEN\",\n",
    "        \"EVERY MORNING I WAKE UP AND START MY DAY, AND\",\n",
    "        \"PEOPLE OFTEN WONDER ABOUT THE MEANING OF LIFE, BUT SELDOM\",\n",
    "        \"COMPUTERS HAVE BECOME ESSENTIAL TO OUR DAILY LIVES, MAKING WORK\",\n",
    "        \"READING BOOKS CAN SIGNIFICANTLY IMPROVE YOUR MIND AND, OVER\",\n",
    "        \"HEALTHY EATING IS NOT JUST ABOUT WEIGHT, IT'S ABOUT\",\n",
    "        \"MUSIC HAS THE POWER TO HEAL AND INSPIRE PEOPLE, WHICH\",\n",
    "        \"SCIENCE CONTINUALLY PUSHES THE BOUNDARIES OF WHAT WE KNOW, AND\",\n",
    "        \"LEARNING A NEW LANGUAGE OPENS UP A WORLD OF OPPORTUNITIES,\",\n",
    "        \"TRAVELING TO NEW PLACES CAN BROADEN YOUR PERSPECTIVE ON, AND\",\n",
    "        \"VIDEO GAMES OFFER A WAY TO ESCAPE REALITY, BUT ALSO\",\n",
    "        \"COFFEE IN THE MORNING CAN SET THE TONE FOR THE\",\n",
    "        \"FRIENDSHIPS ARE BUILT ON TRUST AND MUTUAL RESPECT, WHICH GROWS\",\n",
    "        \"ART PROVIDES A MIRROR TO SOCIETY, SHOWCASING CULTURAL TRENDS AND\",\n",
    "        \"EDUCATION IS KEY TO UNLOCKING POTENTIAL AND EXPANDING, THEREFORE ALWAYS\",\n",
    "        \"THE INTERNET CONNECTS PEOPLE GLOBALLY, YET ALSO POSES SIGNIFICANT CHALLENGES\",\n",
    "        \"PHYSICAL ACTIVITY IS CRUCIAL FOR MAINTAINING HEALTH AND BOOSTING, ESPECIALLY\",\n",
    "        \"ENVIRONMENTAL CONSERVATION IS ESSENTIAL FOR SUSTAINABILITY AND FUTURE GENERATIONS,\",\n",
    "        \"INVESTING WISELY REQUIRES UNDERSTANDING RISK AND REWARD, WHICH IS NOT\",\n",
    "        \"LEADERSHIP DEMANDS RESPONSIBILITY, VISION, AND THE ABILITY TO NAVIGATE, UNDER\",\n",
    "    ]\n",
    "\n",
    "    gpt2.reset_hooks()\n",
    "    filter_mlp_out = lambda name: (layer == name)\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        with t.no_grad():\n",
    "            cache[hook.name] = act.clone().detach()\n",
    "            # act[:, :, 623] *= 0\n",
    "            act[:, :, 401] = -10.\n",
    "            # act[:, :, 401] = 10.\n",
    "            # act[:, :, 2367] *= 0\n",
    "            return act\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    filter_mlp_out2 = lambda name: ('blocks.7.hook_mlp_out' == name)\n",
    "    def forward_cache_hook2(act, hook):\n",
    "        with t.no_grad():\n",
    "            cache[hook.name] = act.clone().detach()\n",
    "    gpt2.add_hook(filter_mlp_out2, forward_cache_hook2, \"fwd\")\n",
    "\n",
    "    most_frequent_top100_neurons = []\n",
    "    mlp_outs = []\n",
    "    gpt2_next_token = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # sentence = sentence.lower()\n",
    "        # logits, cache = gpt2.run_with_cache(sentence, names_filter=[layer])\n",
    "        logits = gpt2(sentence)\n",
    "        # print('next_token=', [gpt2.to_single_str_token(x.item()) for x in logits.argmax(-1)[:, -1]])\n",
    "        gpt2_next_token.append(gpt2.to_single_str_token(logits.argmax(-1)[:, -1].item()))\n",
    "\n",
    "        last_tokens = cache[layer][:, -1]\n",
    "        mlp_out = cache['blocks.7.hook_mlp_out']\n",
    "        token = gpt2.unembed(gpt2.ln_final(mlp_out)).softmax(dim=-1).argmax(dim=-1)\n",
    "        mlp_outs.append(gpt2.to_single_str_token(token[:, -1].item()))\n",
    "        # print('mlp_out=', gpt2.to_string(token[:, -1]))\n",
    "\n",
    "        top_neurons_val, top_neurons_idx = last_tokens.topk(dim=-1, k=100)\n",
    "        # print(f'{top_neurons_idx.shape=}')\n",
    "        most_frequent_top100_neurons.append(top_neurons_idx.view(-1))\n",
    "        # print(f'{most_frequent_top100_neurons.shape=}')\n",
    "    print(f'{gpt2_next_token=}')\n",
    "    print(f'{mlp_outs=}')\n",
    "    bins = t.cat(most_frequent_top100_neurons, dim=0).bincount().cpu()\n",
    "    print('max activated token: ', bins.argmax()) # 623\n",
    "    plt.bar(t.arange(len(bins)), bins)\n",
    "\n",
    "scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 base\n",
      "---\n",
      "the heart of a blue whale is so small that it can't even be seen\n",
      "THE HEART OF A BLUE WHALE IS SO POWERFUL, THAT IT CAN BE T\n",
      "\n",
      "layer=7 neuron=401 to 0\n",
      "---\n",
      "the heart of a blue whale is so small that it can't even be seen\n",
      "THE HEART OF A BLUE WHALE IS SO FAST, it's hard to believe\n",
      "\n",
      "layer=7 neuron=401 to 10\n",
      "---\n",
      "the heart of a blue whale is so MUCH MORE THAN THE BODY OF\n",
      "THE HEART OF A BLUE WHALE IS SO MUCH MORE THAN A PENNY\n"
     ]
    }
   ],
   "source": [
    "def hook_401(val):\n",
    "    gpt2.reset_hooks()\n",
    "    hk_filter = lambda name: ('blocks.7.mlp.hook_post' == name)\n",
    "    def hk(act, hook):\n",
    "        with t.no_grad():\n",
    "            act[:, :, 401] = val\n",
    "            # act[:, :, 401] = val\n",
    "            # print('messing up')\n",
    "            return act\n",
    "    gpt2.add_hook(hk_filter, hk, \"fwd\")\n",
    "\n",
    "sentence = 'The heart of a blue whale is so'\n",
    "# sentence = 'THIS IS THE BEGINNING OF A GREAT ADVENTURE, WHICH THEN'\n",
    "print('GPT2 base\\n---')\n",
    "gpt2.reset_hooks()\n",
    "autoregressively_generate(gpt2, sentence.lower())\n",
    "autoregressively_generate(gpt2, sentence.upper())\n",
    "\n",
    "print('\\nlayer=7 neuron=401 to 0\\n---')\n",
    "hook_401(0)\n",
    "autoregressively_generate(gpt2, sentence.lower())\n",
    "autoregressively_generate(gpt2, sentence.upper())\n",
    "\n",
    "print('\\nlayer=7 neuron=401 to 10\\n---')\n",
    "hook_401(20)\n",
    "autoregressively_generate(gpt2, sentence.lower())\n",
    "autoregressively_generate(gpt2, sentence.upper())\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BASKETBALL PLAYER ISN'T A GOOD PLAYER.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"THE BASKETBALL PLAYER ISN'T A GOOD PLAYER.\\n\\n\\n\""
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.reset_hooks()\n",
    "sentence = 'The basketball player isn\\'t'\n",
    "sentence = 'THE BASKETBALL PLAYER ISN\\'T'\n",
    "autoregressively_generate(gpt2, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fact vs question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_question_dataset = [\n",
    "    {\n",
    "        \"fact\": \"The heart of a blue whale is so big, a human could swim through the arteries of its heart.\",\n",
    "        \"question\": \"Is it true that a human could swim through the arteries of a blue whale's heart?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Sloths can hold their breath for up to 40\",\n",
    "        \"question\": \"Can sloths really hold their breath for up to 40 minutes?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A snail can sleep for up to three years.\",\n",
    "        \"question\": \"Can a snail sleep for up to three years?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Penguins have knees.\",\n",
    "        \"question\": \"Do penguins really have knees?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The male emperor penguin protects the eggs from the cold.\",\n",
    "        \"question\": \"Is it the male emperor penguin that protects the eggs from the cold?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The largest land animal is the African elephant.\",\n",
    "        \"question\": \"Is the African elephant the largest land animal?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Kangaroos use their tails for balance.\",\n",
    "        \"question\": \"Do kangaroos use their tails for balance?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A group of flamingos is called a 'flamboyance.'\",\n",
    "        \"question\": \"Is a group of flamingos called a 'flamboyance'?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Rabbits and hares are different species.\",\n",
    "        \"question\": \"Are rabbits and hares different species?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Cows have four stomachs.\",\n",
    "        \"question\": \"Do cows have four stomachs?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uppercase\n",
    "get top activations for uppercase vs lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches = []\n",
    "upper_caches = []\n",
    "for pair in fact_question_dataset:\n",
    "    lower = pair[\"fact\"]\n",
    "    upper = lower.upper()\n",
    "    gpt2(lower)\n",
    "    lower_caches.append(cache[\"blocks.0.mlp.hook_post\"])\n",
    "    gpt2(upper)\n",
    "    upper_caches.append(cache[\"blocks.0.mlp.hook_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lower = t.stack([x.amax(dim=(0,1)) for x in lower_caches]).amax(0)\n",
    "print(max_lower)\n",
    "min_upper = t.stack([x.amin(dim=(0,1)) for x in upper_caches]).amin(0)\n",
    "print(min_upper.amax())\n",
    "neuron_idx = (min_upper - max_lower).argmax()\n",
    "print(min_upper[neuron_idx])\n",
    "print(max_lower[neuron_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([111, 110, 110, 110, 106, 105, 104, 102, 102, 102, 102, 102, 102, 102,\n",
      "        102, 102, 102, 102, 102, 102, 102, 102, 102, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        100, 100, 100, 100, 100,  98,  97,  97,  97,  97,  94,  92,  91,  88,\n",
      "         71,  63]),\n",
      "indices=tensor([15663,  1391, 17581,  7686, 23529,  6271,   347,     8,     3,    11,\n",
      "         4037,     2,     1,     6,    10,     0,    12,     4,    22,    13,\n",
      "            9,     5,     7,    59,    29,    61,    57,    56,    27,    55,\n",
      "           53,    25,    51,    49,    48,    23,    47,    45,    21,    43,\n",
      "           44,    46,    20,    41,    24,    50,    52,    26,    54,    42,\n",
      "           40,    19,    39,    28,    58,    14,    60,    30,    62,    38,\n",
      "           18,    75,    73,    17,    35,    71,    72,    36,    74,    37,\n",
      "           70,    34,    69,    68,    16,    33,    67,    66,    32,    65,\n",
      "           64,    15,    31,    63,    80,    79,    78,    77,    76,    81,\n",
      "           85,    84,    83,    82, 15213,    86, 17570,    87,    88,  4843]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in lower_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([166, 163, 161, 161, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
      "        160, 160, 160, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159,\n",
      "        159, 159, 159, 159, 159, 159, 159, 158, 158, 158, 158, 158, 158, 158,\n",
      "        158, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157,\n",
      "        157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 156, 155, 155,\n",
      "        155, 155, 155, 155, 154, 154, 154, 154, 154, 154, 154, 154, 154, 154,\n",
      "        154, 152, 151, 151, 150, 150, 149, 148, 147, 145, 140, 134, 129, 124,\n",
      "        116,  95]),\n",
      "indices=tensor([15663, 17581,  1391,  7686,    12,     5,     9,     6,     4,     2,\n",
      "           10,     3,     8,    11,     7,     1,     0,    28,    27,    25,\n",
      "           13,    24,    23,    26,    14,    21, 23529,    22,    19,    18,\n",
      "           17,    29,    16,    15,    20,    30,    37,    36,    35,    34,\n",
      "           33,    32,    31,    60,    59,    57,    56,    55,    53,    54,\n",
      "           52,    51,    58,    50,    49,    48,  6271,    47,    46,    45,\n",
      "           44,    43,    42,    41,    40,    39,    38,    61,    62,    67,\n",
      "           66,    65,    64,    63,    78,    77,    76,    75,    74,    73,\n",
      "           72,    71,    70,    69,    68,    79,   347,  4037,    81,    80,\n",
      "           82,    83,    84,    85,    86,    87, 15213,    88, 17570,    89]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in upper_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt2(fact_question_dataset[0]['fact']) \n",
    "most_probable_token = t.argmax(output[0, -1]).item()\n",
    "gpt2.to_single_str_token(most_probable_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_neuron(neuron_idx, autoencoder, model, dataset): # dataset is a list of sentences\n",
    "   filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "   gpt2.reset_hooks()\n",
    "   cache = {}\n",
    "   def forward_cache_hook(act, hook):\n",
    "      cache[hook.name] = act.detach()\n",
    "      raise Exception(\"end execution\")\n",
    "   gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "   try:\n",
    "      gpt2(dataset)\n",
    "   except Exception:\n",
    "      pass\n",
    "\n",
    "   activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "   _, sae_activations = autoencoder(activations_for_batch)\n",
    "\n",
    "   activations_per_sentence_and_word = {}\n",
    "\n",
    "   print(sae_activations.shape)\n",
    "   tokenized = gpt2.to_tokens(dataset)\n",
    "   for sentence_idx, sentence in enumerate(dataset):\n",
    "      sentence_tokens = tokenized[sentence_idx]\n",
    "      for token_idx, token in enumerate(sentence_tokens):\n",
    "         activations_per_sentence_and_word[(sentence_idx, token_idx)] = sae_activations[sentence_idx, token_idx, neuron_idx]\n",
    "   return activations_per_sentence_and_word\n",
    "\n",
    "# neuron_0 = monitor_neuron(0, autoencoder=model, model=gpt2, dataset=gpt2_dataset[:32]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 24576])\n"
     ]
    }
   ],
   "source": [
    "# all_neuron_activations = [\n",
    "#     sorted(\n",
    "#         monitor_neuron(i, model, gpt2, gpt2_dataset[:32]['text']).items(),\n",
    "#         key=lambda x: x[1],\n",
    "#         reverse=True\n",
    "#     )\n",
    "# for i in [14000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>',\n",
      " '<|endoftext|>A',\n",
      " '<|endoftext|>A magazine',\n",
      " '<|endoftext|>A magazine supplement',\n",
      " '<|endoftext|>A magazine supplement with',\n",
      " '<|endoftext|>A magazine supplement with an',\n",
      " '<|endoftext|>A magazine supplement with an image',\n",
      " '<|endoftext|>A magazine supplement with an image of',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " 'title',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title '\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Un\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book'\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is pictured\",\n",
      " \"A magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in\",\n",
      " \" magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in Berlin\",\n",
      " \" supplement with an image of Adolf Hitler and the title 'The Unreadable \"\n",
      " \"Book' is pictured in Berlin.\",\n",
      " \" with an image of Adolf Hitler and the title 'The Unreadable Book' is \"\n",
      " 'pictured in Berlin. No',\n",
      " \" an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law',\n",
      " \" image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law bans',\n",
      " \" of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. \"\n",
      " 'No law bans �',\n",
      " \" Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No \"\n",
      " 'law bans “',\n",
      " \" Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law \"\n",
      " 'bans “Me',\n",
      " \" and the title 'The Unreadable Book' is pictured in Berlin. No law bans \"\n",
      " '“Mein',\n",
      " \" the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein \"\n",
      " 'Kamp',\n",
      " \" title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf\",\n",
      " \" 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf�\",\n",
      " \"The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf”\",\n",
      " \" Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in\",\n",
      " \"readable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany\",\n",
      " \" Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany,\",\n",
      " \"' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but\",\n",
      " ' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the',\n",
      " ' pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government',\n",
      " ' in Berlin. No law bans “Mein Kampf” in Germany, but the government of',\n",
      " ' Berlin. No law bans “Mein Kampf” in Germany, but the government of Bav',\n",
      " '. No law bans “Mein Kampf” in Germany, but the government of Bavaria',\n",
      " ' No law bans “Mein Kampf” in Germany, but the government of Bavaria,',\n",
      " ' law bans “Mein Kampf” in Germany, but the government of Bavaria, holds',\n",
      " ' bans “Mein Kampf” in Germany, but the government of Bavaria, holds the',\n",
      " ' “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright',\n",
      " '�Mein Kampf” in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and',\n",
      " 'Mein Kampf” in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and guards',\n",
      " 'in Kampf” in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it',\n",
      " ' Kampf” in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it fer',\n",
      " 'f” in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it feroc',\n",
      " '” in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously',\n",
      " '� in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously.',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously. (',\n",
      " ' Germany, but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas',\n",
      " ', but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter',\n",
      " ' but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter/',\n",
      " ' the government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/RE',\n",
      " ' government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/REUTERS',\n",
      " ' of Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)',\n",
      " ' Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)\\n',\n",
      " 'aria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n',\n",
      " ', holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The',\n",
      " ' holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city',\n",
      " ' the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that',\n",
      " ' copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was',\n",
      " ' and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the',\n",
      " ' guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center',\n",
      " ' it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of',\n",
      " ' ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf',\n",
      " 'ociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler',\n",
      " 'iously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler�',\n",
      " '. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’',\n",
      " ' (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’s',\n",
      " 'Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire',\n",
      " ' Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’s empire is',\n",
      " '/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered',\n",
      " 'REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with',\n",
      " 'UTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders',\n",
      " ')\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders of',\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders of the',\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders of the Nazi',\n",
      " 'The city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders of the Nazi past',\n",
      " ' city that was the center of Adolf Hitler’s empire is littered with '\n",
      " 'reminders of the Nazi past,',\n",
      " ' that was the center of Adolf Hitler’s empire is littered with reminders of '\n",
      " 'the Nazi past, from',\n",
      " ' was the center of Adolf Hitler’s empire is littered with reminders of the '\n",
      " 'Nazi past, from the',\n",
      " ' the center of Adolf Hitler’s empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet',\n",
      " ' center of Adolf Hitler’s empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet holes',\n",
      " ' of Adolf Hitler’s empire is littered with reminders of the Nazi past, from '\n",
      " 'the bullet holes that',\n",
      " ' Adolf Hitler’s empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit',\n",
      " ' Hitler’s empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit the',\n",
      " '’s empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts',\n",
      " '�s empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many',\n",
      " ' empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many buildings',\n",
      " ' is littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to',\n",
      " ' littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to the',\n",
      " ' with reminders of the Nazi past, from the bullet holes that pit the fronts '\n",
      " 'of many buildings to the h']\n"
     ]
    }
   ],
   "source": [
    "# tokenized = gpt2.to_tokens(gpt2_dataset[:32]['text'])\n",
    "# tokens_which_activated_neuron_23 = all_neuron_activations[0][:100]\n",
    "# # print(tokens_which_activated_neuron_23[0])\n",
    "# tokenized_sentences = [\n",
    "#     tokenized[sentence_idx][max(word_idx - 20, 0):word_idx + 1] for (sentence_idx, word_idx), _ in tokens_which_activated_neuron_23\n",
    "# ]\n",
    "# untokenized_sentences = [\n",
    "#     gpt2.to_string(tokenized_sentence) for tokenized_sentence in tokenized_sentences\n",
    "# ]\n",
    "# from pprint import pprint\n",
    "# pprint(untokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "gpt2.reset_hooks()\n",
    "cache = {}\n",
    "def forward_cache_hook(act, hook):\n",
    "    cache[hook.name] = act.detach()\n",
    "    raise Exception(\"end execution\")\n",
    "gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "try:\n",
    "    gpt2(gpt2_dataset[:32]['text'])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "_, sae_activations = model(activations_for_batch)\n",
    "neuron_stats = ((sae_activations > 1e-3).sum((0, 1)) < 10) & ((sae_activations > 1e-3).sum((0, 1)) >= 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
