{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with SAE\n",
    "https://transformer-circuits.pub/2023/monosemantic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformer_lens\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if t.cuda.is_available() else ('mps' if t.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    sae_layer = 0\n",
    "    n_in = 3072\n",
    "    n_hidden = n_in * 8\n",
    "    batch_size = 16\n",
    "    max_context = 600\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2 = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "gpt2_dataset = gpt2.load_sample_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n"
     ]
    }
   ],
   "source": [
    "@t.no_grad()\n",
    "def scope():\n",
    "    logits, cache = gpt2.run_with_cache(\"a black cat\")\n",
    "    print(cache)\n",
    "\n",
    "scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, gpt2, gpt2_dataset):\n",
    "        self.gpt2 = gpt2\n",
    "        self.gpt2_dataset = gpt2_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gpt2_dataset)\n",
    "\n",
    "    # @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.gpt2_dataset[idx]\n",
    "        tokens = self.gpt2.to_tokens(sentence['text'], padding_side=\"right\")\n",
    "        _, activations = self.gpt2.run_with_cache(tokens)\n",
    "        return activations['mlp_out', cfg.sae_layer]\n",
    "    \n",
    "dataset = Dataset(gpt2, gpt2_dataset)\n",
    "train_dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_mlp_out(gpt2):\n",
    "#     filter_mlp_out = lambda name: (\"blocks.0.hook_mlp_out\" == name)\n",
    "#     gpt2.reset_hooks()\n",
    "#     cache = {}\n",
    "#     def forward_cache_hook(act, hook):\n",
    "#         cache[hook.name] = act.detach()\n",
    "#     gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "#     return cache\n",
    "\n",
    "# cache = hook_mlp_out(gpt2)\n",
    "# gpt2(gpt2_dataset[:10]['text'])\n",
    "# cache['blocks.0.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size=32, write_after=1000):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "        raise Exception(\"end execution\")\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    output = []\n",
    "    with t.no_grad():\n",
    "        for slice in tqdm(range(0, len(gpt2_dataset), batch_size)):\n",
    "            print(\"looking at slice\", slice)\n",
    "            if slice % write_after < batch_size and slice > 0:\n",
    "                print(\"writing to disk\")\n",
    "                t.save(t.cat(output, dim=0), f\"activations_{slice}.pt\")\n",
    "                output = []\n",
    "\n",
    "            if slice+batch_size >= len(gpt2_dataset):\n",
    "                sliced_dataset = gpt2_dataset[slice:]['text']\n",
    "            else:\n",
    "                sliced_dataset = gpt2_dataset[slice:slice+batch_size]['text']\n",
    "\n",
    "            try:\n",
    "                gpt2(sliced_dataset)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            activations_for_batch = cache['blocks.0.mlp.hook_post']\n",
    "            output.append(activations_for_batch)\n",
    "    # print(f\"{output=}\")\n",
    "    t.save(t.cat(output, dim=0), f\"activations_final.pt\")\n",
    "\n",
    "# create_dataset(write_after=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [fname for fname in os.listdir('./') if 'new_activations' in fname]\n",
    "activation_tensors = [t.load(fname, mmap=True) for fname in files[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "datasets should not be an empty iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_tensors[idx]\n\u001b[0;32m---> 11\u001b[0m concat \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConcatDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactivation_tensors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(concat, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscope\u001b[39m():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:317\u001b[0m, in \u001b[0;36mConcatDataset.__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(datasets)\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets should not be an empty iterable\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, IterableDataset), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatDataset does not support IterableDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
     ]
    }
   ],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, activation_tensors):\n",
    "        self.activation_tensors = activation_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.activation_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.activation_tensors[idx]\n",
    "\n",
    "concat = t.utils.data.ConcatDataset([Dataset(act) for act in activation_tensors]) \n",
    "train_dataloader = DataLoader(concat, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "def scope():\n",
    "    for batch in train_dataloader:\n",
    "        print(batch[0].shape) # should be (64, 1024, 768)\n",
    "        break\n",
    "scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden):\n",
    "        super(SAE, self).__init__()\n",
    "        self.b_a = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_d = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_e = nn.Parameter(t.zeros(n_hidden))\n",
    "\n",
    "        self.W_e = nn.Linear(n_in, n_hidden, bias=False)\n",
    "        self.W_d = nn.Linear(n_hidden, n_in, bias=False)\n",
    "\n",
    "    def forward(self, act):\n",
    "        x = act + self.b_a\n",
    "        x = self.W_e(x) + self.b_e\n",
    "        x = F.relu(x)\n",
    "        hidden_activations = x\n",
    "        x = self.W_d(x) + self.b_d\n",
    "        return x, hidden_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAE(n_in=cfg.n_in, n_hidden=cfg.n_hidden).to(device)\n",
    "opt = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train(model, opt, dataloader, epochs=100, l1_factor=0.1, wnb=True):\n",
    "    if wnb:\n",
    "        wandb.init(project='SAE')\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch[0].to(device)\n",
    "            logits, hidden_activations = model(batch)\n",
    "            reconstruction_loss = F.mse_loss(logits, batch)\n",
    "            sparsity_loss = hidden_activations.abs().mean()\n",
    "            loss = reconstruction_loss + l1_factor * sparsity_loss\n",
    "            if wnb:\n",
    "                wandb.log({\"reconstruction_loss\": reconstruction_loss.item()})\n",
    "                wandb.log({\"sparsity_loss\": sparsity_loss.item()})\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "# train(model, opt, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.save(model.state_dict(), 'model_intermediate.pt')\n",
    "# model.load_state_dict(t.load('model_intermediate.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart of a blue whale is so small\n",
      "The heart of a blue whale is so small that\n",
      "The heart of a blue whale is so small that it\n",
      "The heart of a blue whale is so small that it can\n",
      "The heart of a blue whale is so small that it can barely\n",
      "THE HEART OF A BLUE WHALE IS SO POWER\n",
      "THE HEART OF A BLUE WHALE IS SO POWERFUL\n",
      "THE HEART OF A BLUE WHALE IS SO POWERFUL,\n",
      "THE HEART OF A BLUE WHALE IS SO POWERFUL, THAT\n",
      "THE HEART OF A BLUE WHALE IS SO POWERFUL, THAT IT\n"
     ]
    }
   ],
   "source": [
    "@t.no_grad()\n",
    "def autoregressively_generate(gpt2, input, n_tokens=5):\n",
    "    for i in range(n_tokens):\n",
    "        logits = gpt2(input)\n",
    "        most_probable_token = logits[0, -1].argmax().item()\n",
    "        output = gpt2.to_single_str_token(most_probable_token)\n",
    "        input += output\n",
    "        print(input)\n",
    "\n",
    "input = \"The heart of a blue whale is so\"\n",
    "autoregressively_generate(gpt2, input)\n",
    "autoregressively_generate(gpt2, input.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sae_hook(gpt2=gpt2, sae=model):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        with t.no_grad():\n",
    "            logits, hidden_activations = sae(act)\n",
    "            cache[hook.name] = hidden_activations.detach()\n",
    "            return logits\n",
    "\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YOU\n"
     ]
    }
   ],
   "source": [
    "input = 'the black cat was eating'.upper()\n",
    "input = 'once uppon a time in a'.upper()\n",
    "input = 'hello, my name is regis, how are'.upper()\n",
    "\n",
    "gpt2.reset_hooks()\n",
    "# names_filter=\n",
    "logits, cache = gpt2.run_with_cache(input)\n",
    "\n",
    "point = cache['blocks.0.hook_mlp_out']\n",
    "# point = cache['blocks.0.hook_resid_post']\n",
    "# point = cache['ln_final.hook_normalized']\n",
    "point = cache['blocks.0.hook_attn_out']\n",
    "point = cache['blocks.0.hook_resid_mid']\n",
    "\n",
    "point = cache['blocks.0.hook_resid_pre'] # earliest uppercase\n",
    "point = cache['blocks.7.hook_mlp_out'] # earliest MLP that is upper? sometime 6 ?\n",
    "\n",
    "\n",
    "tokens = gpt2.unembed(gpt2.ln_final(point)).softmax(dim=-1).argmax(dim=-1)\n",
    "print(gpt2.to_string(tokens[0, -1]))\n",
    "\n",
    "# autoregressively_generate(gpt2, input)\n",
    "# autoregressively_generate(gpt2, input.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token= [' leads']\n",
      "mlp_out= ER\n",
      "next_token= [' i']\n",
      "mlp_out= AR\n",
      "next_token= [' do']\n",
      "mlp_out= ED\n",
      "next_token= [' more']\n",
      "mlp_out= WIND\n",
      "next_token= [' time']\n",
      "mlp_out= TIT\n",
      "next_token= [' health']\n",
      "mlp_out=  OF\n",
      "next_token= [' is']\n",
      "mlp_out= ED\n",
      "next_token= [' we']\n",
      "mlp_out= BOOK\n",
      "next_token= [' but']\n",
      "mlp_out= ERY\n",
      "next_token= [' experience']\n",
      "mlp_out= ON\n",
      "next_token= [' to']\n",
      "mlp_out= ED\n",
      "next_token= [' day']\n",
      "mlp_out= ER\n",
      "next_token= [' from']\n",
      "mlp_out= ING\n",
      "next_token= [' the']\n",
      "mlp_out= ED\n",
      "next_token= [' consider']\n",
      "mlp_out= ER\n",
      "next_token= ['.']\n",
      "mlp_out= AUT\n",
      "next_token= [' in']\n",
      "mlp_out= ED\n",
      "next_token= [' and']\n",
      "mlp_out= ER\n",
      "next_token= [' only']\n",
      "mlp_out= OR\n",
      "next_token= [' the']\n",
      "mlp_out= LIN\n",
      "tensor(2367)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scope(layer='blocks.7.mlp.hook_post'):\n",
    "    sentences = [\n",
    "        \"THIS IS THE BEGINNING OF A GREAT ADVENTURE, WHICH THEN\",\n",
    "        \"EVERY MORNING I WAKE UP AND START MY DAY, AND\",\n",
    "        \"PEOPLE OFTEN WONDER ABOUT THE MEANING OF LIFE, BUT SELDOM\",\n",
    "        \"COMPUTERS HAVE BECOME ESSENTIAL TO OUR DAILY LIVES, MAKING WORK\",\n",
    "        \"READING BOOKS CAN SIGNIFICANTLY IMPROVE YOUR MIND AND, OVER\",\n",
    "        \"HEALTHY EATING IS NOT JUST ABOUT WEIGHT, IT'S ABOUT\",\n",
    "        \"MUSIC HAS THE POWER TO HEAL AND INSPIRE PEOPLE, WHICH\",\n",
    "        \"SCIENCE CONTINUALLY PUSHES THE BOUNDARIES OF WHAT WE KNOW, AND\",\n",
    "        \"LEARNING A NEW LANGUAGE OPENS UP A WORLD OF OPPORTUNITIES,\",\n",
    "        \"TRAVELING TO NEW PLACES CAN BROADEN YOUR PERSPECTIVE ON, AND\",\n",
    "        \"VIDEO GAMES OFFER A WAY TO ESCAPE REALITY, BUT ALSO\",\n",
    "        \"COFFEE IN THE MORNING CAN SET THE TONE FOR THE\",\n",
    "        \"FRIENDSHIPS ARE BUILT ON TRUST AND MUTUAL RESPECT, WHICH GROWS\",\n",
    "        \"ART PROVIDES A MIRROR TO SOCIETY, SHOWCASING CULTURAL TRENDS AND\",\n",
    "        \"EDUCATION IS KEY TO UNLOCKING POTENTIAL AND EXPANDING, THEREFORE ALWAYS\",\n",
    "        \"THE INTERNET CONNECTS PEOPLE GLOBALLY, YET ALSO POSES SIGNIFICANT CHALLENGES\",\n",
    "        \"PHYSICAL ACTIVITY IS CRUCIAL FOR MAINTAINING HEALTH AND BOOSTING, ESPECIALLY\",\n",
    "        \"ENVIRONMENTAL CONSERVATION IS ESSENTIAL FOR SUSTAINABILITY AND FUTURE GENERATIONS,\",\n",
    "        \"INVESTING WISELY REQUIRES UNDERSTANDING RISK AND REWARD, WHICH IS NOT\",\n",
    "        \"LEADERSHIP DEMANDS RESPONSIBILITY, VISION, AND THE ABILITY TO NAVIGATE, UNDER\",\n",
    "    ]\n",
    "\n",
    "    gpt2.reset_hooks()\n",
    "    filter_mlp_out = lambda name: (layer == name)\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        with t.no_grad():\n",
    "            cache[hook.name] = act.clone().detach()\n",
    "            # act[:, :, 623] *= 0\n",
    "            act[:, :, 401] = 10.\n",
    "            return act\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    filter_mlp_out2 = lambda name: ('blocks.7.hook_mlp_out' == name)\n",
    "    def forward_cache_hook2(act, hook):\n",
    "        with t.no_grad():\n",
    "            cache[hook.name] = act.clone().detach()\n",
    "    gpt2.add_hook(filter_mlp_out2, forward_cache_hook2, \"fwd\")\n",
    "\n",
    "    most_frequent_top100_neurons = []\n",
    "    mlp_outs = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        # logits, cache = gpt2.run_with_cache(sentence, names_filter=[layer])\n",
    "        logits = gpt2(sentence)\n",
    "        print('next_token=', [gpt2.to_single_str_token(x.item()) for x in logits.argmax(-1)[:, -1]])\n",
    "\n",
    "        last_tokens = cache[layer][:, -1]\n",
    "        mlp_out = cache['blocks.7.hook_mlp_out']\n",
    "        # mlp_outs.append(mlp_out)\n",
    "        token = gpt2.unembed(gpt2.ln_final(mlp_out)).softmax(dim=-1).argmax(dim=-1)\n",
    "        print('mlp_out=', gpt2.to_string(token[:, -1]))\n",
    "\n",
    "\n",
    "        top_neurons_val, top_neurons_idx = last_tokens.topk(dim=-1, k=100)\n",
    "        # print(f'{top_neurons_idx.shape=}')\n",
    "        most_frequent_top100_neurons.append(top_neurons_idx.view(-1))\n",
    "        # print(f'{most_frequent_top100_neurons.shape=}')\n",
    "    bins = t.cat(most_frequent_top100_neurons, dim=0).bincount().cpu()\n",
    "    print(bins.argmax()) # 623\n",
    "    plt.bar(t.arange(len(bins)), bins)\n",
    "\n",
    "scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for i in t.ones(1, 3, 4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fact vs question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_question_dataset = [\n",
    "    {\n",
    "        \"fact\": \"The heart of a blue whale is so big, a human could swim through the arteries of its heart.\",\n",
    "        \"question\": \"Is it true that a human could swim through the arteries of a blue whale's heart?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Sloths can hold their breath for up to 40\",\n",
    "        \"question\": \"Can sloths really hold their breath for up to 40 minutes?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A snail can sleep for up to three years.\",\n",
    "        \"question\": \"Can a snail sleep for up to three years?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Penguins have knees.\",\n",
    "        \"question\": \"Do penguins really have knees?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The male emperor penguin protects the eggs from the cold.\",\n",
    "        \"question\": \"Is it the male emperor penguin that protects the eggs from the cold?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The largest land animal is the African elephant.\",\n",
    "        \"question\": \"Is the African elephant the largest land animal?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Kangaroos use their tails for balance.\",\n",
    "        \"question\": \"Do kangaroos use their tails for balance?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A group of flamingos is called a 'flamboyance.'\",\n",
    "        \"question\": \"Is a group of flamingos called a 'flamboyance'?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Rabbits and hares are different species.\",\n",
    "        \"question\": \"Are rabbits and hares different species?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Cows have four stomachs.\",\n",
    "        \"question\": \"Do cows have four stomachs?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uppercase\n",
    "get top activations for uppercase vs lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches = []\n",
    "upper_caches = []\n",
    "for pair in fact_question_dataset:\n",
    "    lower = pair[\"fact\"]\n",
    "    upper = lower.upper()\n",
    "    gpt2(lower)\n",
    "    lower_caches.append(cache[\"blocks.0.mlp.hook_post\"])\n",
    "    gpt2(upper)\n",
    "    upper_caches.append(cache[\"blocks.0.mlp.hook_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lower = t.stack([x.amax(dim=(0,1)) for x in lower_caches]).amax(0)\n",
    "print(max_lower)\n",
    "min_upper = t.stack([x.amin(dim=(0,1)) for x in upper_caches]).amin(0)\n",
    "print(min_upper.amax())\n",
    "neuron_idx = (min_upper - max_lower).argmax()\n",
    "print(min_upper[neuron_idx])\n",
    "print(max_lower[neuron_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([111, 110, 110, 110, 106, 105, 104, 102, 102, 102, 102, 102, 102, 102,\n",
      "        102, 102, 102, 102, 102, 102, 102, 102, 102, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        100, 100, 100, 100, 100,  98,  97,  97,  97,  97,  94,  92,  91,  88,\n",
      "         71,  63]),\n",
      "indices=tensor([15663,  1391, 17581,  7686, 23529,  6271,   347,     8,     3,    11,\n",
      "         4037,     2,     1,     6,    10,     0,    12,     4,    22,    13,\n",
      "            9,     5,     7,    59,    29,    61,    57,    56,    27,    55,\n",
      "           53,    25,    51,    49,    48,    23,    47,    45,    21,    43,\n",
      "           44,    46,    20,    41,    24,    50,    52,    26,    54,    42,\n",
      "           40,    19,    39,    28,    58,    14,    60,    30,    62,    38,\n",
      "           18,    75,    73,    17,    35,    71,    72,    36,    74,    37,\n",
      "           70,    34,    69,    68,    16,    33,    67,    66,    32,    65,\n",
      "           64,    15,    31,    63,    80,    79,    78,    77,    76,    81,\n",
      "           85,    84,    83,    82, 15213,    86, 17570,    87,    88,  4843]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in lower_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([166, 163, 161, 161, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
      "        160, 160, 160, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159,\n",
      "        159, 159, 159, 159, 159, 159, 159, 158, 158, 158, 158, 158, 158, 158,\n",
      "        158, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157,\n",
      "        157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 156, 155, 155,\n",
      "        155, 155, 155, 155, 154, 154, 154, 154, 154, 154, 154, 154, 154, 154,\n",
      "        154, 152, 151, 151, 150, 150, 149, 148, 147, 145, 140, 134, 129, 124,\n",
      "        116,  95]),\n",
      "indices=tensor([15663, 17581,  1391,  7686,    12,     5,     9,     6,     4,     2,\n",
      "           10,     3,     8,    11,     7,     1,     0,    28,    27,    25,\n",
      "           13,    24,    23,    26,    14,    21, 23529,    22,    19,    18,\n",
      "           17,    29,    16,    15,    20,    30,    37,    36,    35,    34,\n",
      "           33,    32,    31,    60,    59,    57,    56,    55,    53,    54,\n",
      "           52,    51,    58,    50,    49,    48,  6271,    47,    46,    45,\n",
      "           44,    43,    42,    41,    40,    39,    38,    61,    62,    67,\n",
      "           66,    65,    64,    63,    78,    77,    76,    75,    74,    73,\n",
      "           72,    71,    70,    69,    68,    79,   347,  4037,    81,    80,\n",
      "           82,    83,    84,    85,    86,    87, 15213,    88, 17570,    89]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in upper_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt2(fact_question_dataset[0]['fact']) \n",
    "most_probable_token = t.argmax(output[0, -1]).item()\n",
    "gpt2.to_single_str_token(most_probable_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_neuron(neuron_idx, autoencoder, model, dataset): # dataset is a list of sentences\n",
    "   filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "   gpt2.reset_hooks()\n",
    "   cache = {}\n",
    "   def forward_cache_hook(act, hook):\n",
    "      cache[hook.name] = act.detach()\n",
    "      raise Exception(\"end execution\")\n",
    "   gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "   try:\n",
    "      gpt2(dataset)\n",
    "   except Exception:\n",
    "      pass\n",
    "\n",
    "   activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "   _, sae_activations = autoencoder(activations_for_batch)\n",
    "\n",
    "   activations_per_sentence_and_word = {}\n",
    "\n",
    "   print(sae_activations.shape)\n",
    "   tokenized = gpt2.to_tokens(dataset)\n",
    "   for sentence_idx, sentence in enumerate(dataset):\n",
    "      sentence_tokens = tokenized[sentence_idx]\n",
    "      for token_idx, token in enumerate(sentence_tokens):\n",
    "         activations_per_sentence_and_word[(sentence_idx, token_idx)] = sae_activations[sentence_idx, token_idx, neuron_idx]\n",
    "   return activations_per_sentence_and_word\n",
    "\n",
    "# neuron_0 = monitor_neuron(0, autoencoder=model, model=gpt2, dataset=gpt2_dataset[:32]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 24576])\n"
     ]
    }
   ],
   "source": [
    "# all_neuron_activations = [\n",
    "#     sorted(\n",
    "#         monitor_neuron(i, model, gpt2, gpt2_dataset[:32]['text']).items(),\n",
    "#         key=lambda x: x[1],\n",
    "#         reverse=True\n",
    "#     )\n",
    "# for i in [14000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>',\n",
      " '<|endoftext|>A',\n",
      " '<|endoftext|>A magazine',\n",
      " '<|endoftext|>A magazine supplement',\n",
      " '<|endoftext|>A magazine supplement with',\n",
      " '<|endoftext|>A magazine supplement with an',\n",
      " '<|endoftext|>A magazine supplement with an image',\n",
      " '<|endoftext|>A magazine supplement with an image of',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " 'title',\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title '\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Un\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book'\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is\",\n",
      " '<|endoftext|>A magazine supplement with an image of Adolf Hitler and the '\n",
      " \"title 'The Unreadable Book' is pictured\",\n",
      " \"A magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in\",\n",
      " \" magazine supplement with an image of Adolf Hitler and the title 'The \"\n",
      " \"Unreadable Book' is pictured in Berlin\",\n",
      " \" supplement with an image of Adolf Hitler and the title 'The Unreadable \"\n",
      " \"Book' is pictured in Berlin.\",\n",
      " \" with an image of Adolf Hitler and the title 'The Unreadable Book' is \"\n",
      " 'pictured in Berlin. No',\n",
      " \" an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law',\n",
      " \" image of Adolf Hitler and the title 'The Unreadable Book' is pictured in \"\n",
      " 'Berlin. No law bans',\n",
      " \" of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. \"\n",
      " 'No law bans ',\n",
      " \" Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No \"\n",
      " 'law bans ',\n",
      " \" Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law \"\n",
      " 'bans Me',\n",
      " \" and the title 'The Unreadable Book' is pictured in Berlin. No law bans \"\n",
      " 'Mein',\n",
      " \" the title 'The Unreadable Book' is pictured in Berlin. No law bans Mein \"\n",
      " 'Kamp',\n",
      " \" title 'The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \" 'The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \"The Unreadable Book' is pictured in Berlin. No law bans Mein Kampf\",\n",
      " \" Unreadable Book' is pictured in Berlin. No law bans Mein Kampf in\",\n",
      " \"readable Book' is pictured in Berlin. No law bans Mein Kampf in Germany\",\n",
      " \" Book' is pictured in Berlin. No law bans Mein Kampf in Germany,\",\n",
      " \"' is pictured in Berlin. No law bans Mein Kampf in Germany, but\",\n",
      " ' is pictured in Berlin. No law bans Mein Kampf in Germany, but the',\n",
      " ' pictured in Berlin. No law bans Mein Kampf in Germany, but the government',\n",
      " ' in Berlin. No law bans Mein Kampf in Germany, but the government of',\n",
      " ' Berlin. No law bans Mein Kampf in Germany, but the government of Bav',\n",
      " '. No law bans Mein Kampf in Germany, but the government of Bavaria',\n",
      " ' No law bans Mein Kampf in Germany, but the government of Bavaria,',\n",
      " ' law bans Mein Kampf in Germany, but the government of Bavaria, holds',\n",
      " ' bans Mein Kampf in Germany, but the government of Bavaria, holds the',\n",
      " ' Mein Kampf in Germany, but the government of Bavaria, holds the copyright',\n",
      " 'Mein Kampf in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and',\n",
      " 'Mein Kampf in Germany, but the government of Bavaria, holds the copyright '\n",
      " 'and guards',\n",
      " 'in Kampf in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it',\n",
      " ' Kampf in Germany, but the government of Bavaria, holds the copyright and '\n",
      " 'guards it fer',\n",
      " 'f in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it feroc',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously.',\n",
      " ' in Germany, but the government of Bavaria, holds the copyright and guards '\n",
      " 'it ferociously. (',\n",
      " ' Germany, but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas',\n",
      " ', but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter',\n",
      " ' but the government of Bavaria, holds the copyright and guards it '\n",
      " 'ferociously. (Thomas Peter/',\n",
      " ' the government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/RE',\n",
      " ' government of Bavaria, holds the copyright and guards it ferociously. '\n",
      " '(Thomas Peter/REUTERS',\n",
      " ' of Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)',\n",
      " ' Bavaria, holds the copyright and guards it ferociously. (Thomas '\n",
      " 'Peter/REUTERS)\\n',\n",
      " 'aria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n',\n",
      " ', holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The',\n",
      " ' holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city',\n",
      " ' the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that',\n",
      " ' copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was',\n",
      " ' and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the',\n",
      " ' guards it ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center',\n",
      " ' it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of',\n",
      " ' ferociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf',\n",
      " 'ociously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler',\n",
      " 'iously. (Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitler',\n",
      " '. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler',\n",
      " ' (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitlers',\n",
      " 'Thomas Peter/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire',\n",
      " ' Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitlers empire is',\n",
      " '/REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered',\n",
      " 'REUTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with',\n",
      " 'UTERS)\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders',\n",
      " ')\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of',\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the',\n",
      " '\\n'\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi',\n",
      " 'The city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi past',\n",
      " ' city that was the center of Adolf Hitlers empire is littered with '\n",
      " 'reminders of the Nazi past,',\n",
      " ' that was the center of Adolf Hitlers empire is littered with reminders of '\n",
      " 'the Nazi past, from',\n",
      " ' was the center of Adolf Hitlers empire is littered with reminders of the '\n",
      " 'Nazi past, from the',\n",
      " ' the center of Adolf Hitlers empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet',\n",
      " ' center of Adolf Hitlers empire is littered with reminders of the Nazi '\n",
      " 'past, from the bullet holes',\n",
      " ' of Adolf Hitlers empire is littered with reminders of the Nazi past, from '\n",
      " 'the bullet holes that',\n",
      " ' Adolf Hitlers empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit',\n",
      " ' Hitlers empire is littered with reminders of the Nazi past, from the '\n",
      " 'bullet holes that pit the',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of',\n",
      " 's empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many',\n",
      " ' empire is littered with reminders of the Nazi past, from the bullet holes '\n",
      " 'that pit the fronts of many buildings',\n",
      " ' is littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to',\n",
      " ' littered with reminders of the Nazi past, from the bullet holes that pit '\n",
      " 'the fronts of many buildings to the',\n",
      " ' with reminders of the Nazi past, from the bullet holes that pit the fronts '\n",
      " 'of many buildings to the h']\n"
     ]
    }
   ],
   "source": [
    "# tokenized = gpt2.to_tokens(gpt2_dataset[:32]['text'])\n",
    "# tokens_which_activated_neuron_23 = all_neuron_activations[0][:100]\n",
    "# # print(tokens_which_activated_neuron_23[0])\n",
    "# tokenized_sentences = [\n",
    "#     tokenized[sentence_idx][max(word_idx - 20, 0):word_idx + 1] for (sentence_idx, word_idx), _ in tokens_which_activated_neuron_23\n",
    "# ]\n",
    "# untokenized_sentences = [\n",
    "#     gpt2.to_string(tokenized_sentence) for tokenized_sentence in tokenized_sentences\n",
    "# ]\n",
    "# from pprint import pprint\n",
    "# pprint(untokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "gpt2.reset_hooks()\n",
    "cache = {}\n",
    "def forward_cache_hook(act, hook):\n",
    "    cache[hook.name] = act.detach()\n",
    "    raise Exception(\"end execution\")\n",
    "gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "try:\n",
    "    gpt2(gpt2_dataset[:32]['text'])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "activations_for_batch = cache['blocks.0.mlp.hook_post'] # [batch, ctx, d_in]\n",
    "_, sae_activations = model(activations_for_batch)\n",
    "neuron_stats = ((sae_activations > 1e-3).sum((0, 1)) < 10) & ((sae_activations > 1e-3).sum((0, 1)) >= 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
