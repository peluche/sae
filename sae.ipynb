{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment with SAE\n",
    "https://transformer-circuits.pub/2023/monosemantic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformer_lens\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "# print(t.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    sae_layer = 0\n",
    "    n_in = 3072\n",
    "    n_hidden = n_in * 8\n",
    "    batch_size = 16\n",
    "    max_context = 600\n",
    "\n",
    "cfg = CFG()\n",
    "print(cfg.n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "# logits, activations = gpt2.run_with_cache(\"Hello World\")\n",
    "gpt2_dataset = gpt2.load_sample_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = gpt2.run_with_cache(\"a black cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, gpt2, gpt2_dataset):\n",
    "        self.gpt2 = gpt2\n",
    "        self.gpt2_dataset = gpt2_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gpt2_dataset)\n",
    "\n",
    "    # @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.gpt2_dataset[idx]\n",
    "        tokens = self.gpt2.to_tokens(sentence['text'], padding_side=\"right\")\n",
    "        _, activations = self.gpt2.run_with_cache(tokens)\n",
    "        return activations['mlp_out', cfg.sae_layer]\n",
    "    \n",
    "dataset = Dataset(gpt2, gpt2_dataset)\n",
    "train_dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = gpt2.to_tokens([gpt2_dataset[1002]['text'], gpt2_dataset[0]['text']], padding_side=\"right\")\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_mlp_out(gpt2):\n",
    "#     filter_mlp_out = lambda name: (\"blocks.0.hook_mlp_out\" == name)\n",
    "#     gpt2.reset_hooks()\n",
    "#     cache = {}\n",
    "#     def forward_cache_hook(act, hook):\n",
    "#         cache[hook.name] = act.detach()\n",
    "#     gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "#     return cache\n",
    "\n",
    "# cache = hook_mlp_out(gpt2)\n",
    "# gpt2(gpt2_dataset[:10]['text'])\n",
    "# cache['blocks.0.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size=32, write_after=1000):\n",
    "    filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "    gpt2.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "        raise Exception(\"end execution\")\n",
    "    gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    output = []\n",
    "    with t.no_grad():\n",
    "        for slice in tqdm(range(0, len(gpt2_dataset), batch_size)):\n",
    "            print(\"looking at slice\", slice)\n",
    "            if slice % write_after < batch_size and slice > 0:\n",
    "                print(\"writing to disk\")\n",
    "                t.save(t.cat(output, dim=0), f\"activations_{slice}.pt\")\n",
    "                output = []\n",
    "\n",
    "            if slice+batch_size >= len(gpt2_dataset):\n",
    "                sliced_dataset = gpt2_dataset[slice:]['text']\n",
    "            else:\n",
    "                sliced_dataset = gpt2_dataset[slice:slice+batch_size]['text']\n",
    "\n",
    "            try:\n",
    "                gpt2(sliced_dataset)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            activations_for_batch = cache['blocks.0.mlp.hook_post']\n",
    "            output.append(activations_for_batch)\n",
    "    # print(f\"{output=}\")\n",
    "    t.save(t.cat(output, dim=0), f\"activations_final.pt\")\n",
    "\n",
    "# create_dataset(write_after=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [fname for fname in os.listdir('./') if 'activations' in fname]\n",
    "activation_tensors = [t.load(fname) for fname in files[:5]]\n",
    "activation_tensors = t.cat(activation_tensors, dim=0)\n",
    "print(activation_tensors.shape) # should be (10000, 1024, 768)\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, activation_tensors):\n",
    "        self.activation_tensors = activation_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.activation_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.activation_tensors[idx]\n",
    "    \n",
    "train_dataloader = DataLoader(t.utils.data.TensorDataset(activation_tensors), batch_size=cfg.batch_size, shuffle=True)\n",
    "for batch in train_dataloader:\n",
    "    print(batch[0].shape) # should be (1, 1024, 768)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden):\n",
    "        super(SAE, self).__init__()\n",
    "        self.b_a = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_d = nn.Parameter(t.zeros(n_in))\n",
    "        self.b_e = nn.Parameter(t.zeros(n_hidden))\n",
    "\n",
    "        self.W_e = nn.Linear(n_in, n_hidden, bias=False)\n",
    "        self.W_d = nn.Linear(n_hidden, n_in, bias=False)\n",
    "\n",
    "    def forward(self, act):\n",
    "        x = act + self.b_a\n",
    "        x = self.W_e(x) + self.b_e\n",
    "        x = F.relu(x)\n",
    "        hidden_activations = x\n",
    "        x = self.W_d(x) + self.b_d\n",
    "        return x, hidden_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAE(n_in=cfg.n_in, n_hidden=cfg.n_hidden).to(device)\n",
    "opt = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train(model, opt, dataloader, epochs=100, l1_factor=0.1, wnb=True):\n",
    "    if wnb:\n",
    "        wandb.init(project='SAE')\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch[0].to(device)\n",
    "            logits, hidden_activations = model(batch)\n",
    "            reconstruction_loss = F.mse_loss(logits, batch)\n",
    "            sparsity_loss = hidden_activations.abs().mean()\n",
    "            loss = reconstruction_loss + l1_factor * sparsity_loss\n",
    "            if wnb:\n",
    "                wandb.log({\"reconstruction_loss\": reconstruction_loss.item()})\n",
    "                wandb.log({\"sparsity_loss\": sparsity_loss.item()})\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        # TODO: validation loss\n",
    "        # if wnb:\n",
    "            # wandb.log({\"epoch\": epoch})\n",
    "\n",
    "train(model, opt, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(), 'model_intermediate.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"a\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"The heart of a blue whale is so\" #.upper()\n",
    "\n",
    "for i in range(10):\n",
    "    logits = gpt2(input)\n",
    "    most_probable_token = logits[0, -1].argmax().item()\n",
    "    output = gpt2.to_single_str_token(most_probable_token)\n",
    "    input += output\n",
    "    print(input)\n",
    "\n",
    "# logits = gpt2(\"THE HEART OF A BLUE WHALE IS A BL\")\n",
    "# print(logits.shape)\n",
    "# most_probable_token = logits[0, -1].argmax().item()\n",
    "# gpt2.to_single_str_token(most_probable_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_question_dataset = [\n",
    "    {\n",
    "        \"fact\": \"The heart of a blue whale is so big, a human could swim through the arteries of its heart.\",\n",
    "        \"question\": \"Is it true that a human could swim through the arteries of a blue whale's heart?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Sloths can hold their breath for up to 40\",\n",
    "        \"question\": \"Can sloths really hold their breath for up to 40 minutes?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A snail can sleep for up to three years.\",\n",
    "        \"question\": \"Can a snail sleep for up to three years?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Penguins have knees.\",\n",
    "        \"question\": \"Do penguins really have knees?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The male emperor penguin protects the eggs from the cold.\",\n",
    "        \"question\": \"Is it the male emperor penguin that protects the eggs from the cold?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"The largest land animal is the African elephant.\",\n",
    "        \"question\": \"Is the African elephant the largest land animal?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Kangaroos use their tails for balance.\",\n",
    "        \"question\": \"Do kangaroos use their tails for balance?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"A group of flamingos is called a 'flamboyance.'\",\n",
    "        \"question\": \"Is a group of flamingos called a 'flamboyance'?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Rabbits and hares are different species.\",\n",
    "        \"question\": \"Are rabbits and hares different species?\"\n",
    "    },\n",
    "    {\n",
    "        \"fact\": \"Cows have four stomachs.\",\n",
    "        \"question\": \"Do cows have four stomachs?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mlp_out = lambda name: (\"blocks.0.mlp.hook_post\" == name)\n",
    "gpt2.reset_hooks()\n",
    "cache = {}\n",
    "def forward_cache_hook(act, hook):\n",
    "    with t.no_grad():\n",
    "        logits, hidden_activations = model(act)\n",
    "        cache[hook.name] = hidden_activations.detach()\n",
    "        return logits\n",
    "\n",
    "gpt2.add_hook(filter_mlp_out, forward_cache_hook, \"fwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches = []\n",
    "upper_caches = []\n",
    "for pair in fact_question_dataset:\n",
    "    lower = pair[\"fact\"]\n",
    "    upper = lower.upper()\n",
    "    gpt2(lower)\n",
    "    lower_caches.append(cache[\"blocks.0.mlp.hook_post\"])\n",
    "    gpt2(upper)\n",
    "    upper_caches.append(cache[\"blocks.0.mlp.hook_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_caches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lower = t.stack([x.amax(dim=(0,1)) for x in lower_caches]).amax(0)\n",
    "print(max_lower)\n",
    "min_upper = t.stack([x.amin(dim=(0,1)) for x in upper_caches]).amin(0)\n",
    "print(min_upper.amax())\n",
    "neuron_idx = (min_upper - max_lower).argmax()\n",
    "print(min_upper[neuron_idx])\n",
    "print(max_lower[neuron_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.rand(3,5)\n",
    "print(a)\n",
    "print(a.topk(2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([111, 110, 110, 110, 106, 105, 104, 102, 102, 102, 102, 102, 102, 102,\n",
      "        102, 102, 102, 102, 102, 102, 102, 102, 102, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
      "        100, 100, 100, 100, 100,  98,  97,  97,  97,  97,  94,  92,  91,  88,\n",
      "         71,  63]),\n",
      "indices=tensor([15663,  1391, 17581,  7686, 23529,  6271,   347,     8,     3,    11,\n",
      "         4037,     2,     1,     6,    10,     0,    12,     4,    22,    13,\n",
      "            9,     5,     7,    59,    29,    61,    57,    56,    27,    55,\n",
      "           53,    25,    51,    49,    48,    23,    47,    45,    21,    43,\n",
      "           44,    46,    20,    41,    24,    50,    52,    26,    54,    42,\n",
      "           40,    19,    39,    28,    58,    14,    60,    30,    62,    38,\n",
      "           18,    75,    73,    17,    35,    71,    72,    36,    74,    37,\n",
      "           70,    34,    69,    68,    16,    33,    67,    66,    32,    65,\n",
      "           64,    15,    31,    63,    80,    79,    78,    77,    76,    81,\n",
      "           85,    84,    83,    82, 15213,    86, 17570,    87,    88,  4843]))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in lower_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([166, 163, 161, 161, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
      "        160, 160, 160, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159,\n",
      "        159, 159, 159, 159, 159, 159, 159, 158, 158, 158, 158, 158, 158, 158,\n",
      "        158, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157,\n",
      "        157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 156, 155, 155,\n",
      "        155, 155, 155, 155, 154, 154, 154, 154, 154, 154, 154, 154, 154, 154,\n",
      "        154, 152, 151, 151, 150, 150, 149, 148, 147, 145, 140, 134, 129, 124,\n",
      "        116,  95]),\n",
      "indices=tensor([15663, 17581,  1391,  7686,    12,     5,     9,     6,     4,     2,\n",
      "           10,     3,     8,    11,     7,     1,     0,    28,    27,    25,\n",
      "           13,    24,    23,    26,    14,    21, 23529,    22,    19,    18,\n",
      "           17,    29,    16,    15,    20,    30,    37,    36,    35,    34,\n",
      "           33,    32,    31,    60,    59,    57,    56,    55,    53,    54,\n",
      "           52,    51,    58,    50,    49,    48,  6271,    47,    46,    45,\n",
      "           44,    43,    42,    41,    40,    39,    38,    61,    62,    67,\n",
      "           66,    65,    64,    63,    78,    77,    76,    75,    74,    73,\n",
      "           72,    71,    70,    69,    68,    79,   347,  4037,    81,    80,\n",
      "           82,    83,    84,    85,    86,    87, 15213,    88, 17570,    89]))\n"
     ]
    }
   ],
   "source": [
    "frequency_neuron_counter = {}\n",
    "flat_count = []\n",
    "for l in upper_caches:\n",
    "    v, i= l.topk(100, dim=2)\n",
    "    flat_count.append(i.view(-1))\n",
    "bincount = t.cat(flat_count, dim=0).bincount().cpu()\n",
    "print(bincount.topk(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = gpt2(fact_question_dataset[0]['fact']) \n",
    "\n",
    "most_probable_token = t.argmax(output[0, -1]).item()\n",
    "gpt2.to_single_str_token(most_probable_token)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
